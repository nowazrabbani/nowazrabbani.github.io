---
title: "Patch-level Routing in Mixture-of-Experts is Provably Sample-Efficient for Convolutional Neural Networks"
year: 2023
venue: ICML (Oral Presentation, top 8.5%)
authors:
  - Mohammed Nowaz Rabbani Chowdhury
  - Shuai Zhang
  - Meng Wang
  - Sijia Liu
  - Pin-Yu Chen
---

## Abstract
This paper provides the first **theoretical analysis of patch-level routing** in sparse Mixture-of-Experts (MoE) models for convolutional neural networks (CNNs).  
We show that patch-level routing introduces **sample-efficiency benefits** under realistic assumptions by directing computation only to the most relevant experts.

The work bridges the gap between MoE theory and practice by:
- describing routing dynamics analytically,
- proving sample-efficient behavior of MoE under convolutional settings,
- validating the theory through extensive experiments.

This paper was **accepted at ICML 2023** with an **Oral Presentation** (top 8.5% of accepted papers).

## Key Contributions
- First theoretical analysis of **patch-level MoE routing** for CNNs  
- Established sample-efficiency guarantees  
- Demonstrated strong empirical performance in vision tasks  
- Provided insights into routing behavior and generalization

## Publication
Published in **International Conference on Machine Learning (ICML), 2023**, pp. 6074â€“6114.

## Citation
