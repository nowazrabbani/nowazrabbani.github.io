---
title: "Efficient Training of Sparse Mixture-of-Experts (MoE)"
---

- First theoretical characterization of MoE training dynamics  
- Identified key efficiency-driving properties  
- Demonstrated data-efficient training (ICML 2023 Oral)  
