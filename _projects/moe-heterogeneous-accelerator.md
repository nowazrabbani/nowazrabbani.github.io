---
title: "Optimized Deployment of Large Mixture-of-Experts (MoE) Models on Heterogeneous Analog–Digital AI Accelerators"
---

## Overview
This project develops an **efficient deployment framework** for large Mixture-of-Experts (MoE) models on **heterogeneous Analog–Digital AI accelerators**.  
Such systems combine **analog in-memory computing** with traditional **digital compute units**, enabling extremely high throughput but requiring specialized model optimization techniques.

The goal is to take advantage of analog efficiency while maintaining digital precision when needed, all with **theoretical guarantees** on
